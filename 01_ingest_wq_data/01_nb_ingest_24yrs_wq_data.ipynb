{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c845a68-6e34-4e5a-8f9b-7cb6ec008d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Summary:\n",
    "\n",
    "# V.History: \n",
    "# Date Last Modified: 14 May 2025\n",
    "\n",
    "# EA has provided one file per year of WQ observations, from 2000 to 2024, totalling 24 files.\n",
    "# This Notebook reads all 24 years of WQ (EA's open-sourced data) and\n",
    "# combines them into one dataset per five years.\n",
    "# This approach avoids GCP's memory issues when processing large files if all 24 files are read at once.\n",
    "\n",
    "# Output File Locations of this notebook\n",
    "# 1) gcs://rdmai_data/cleansed/02_WQEA_2000_2004_Raw_New.csv\n",
    "# 2) gcs://rdmai_data/cleansed/02_WQEA_2005_2009_Raw_New.csv\n",
    "# 3) gcs://rdmai_data/cleansed/02_WQEA_2010_2014_Raw_New.csv\n",
    "# 4) gcs://rdmai_data/cleansed/02_WQEA_2015_2019_Raw_New.csv\n",
    "# 5) gcs://rdmai_data/cleansed/02_WQEA_2020_2024_Raw_New.csv\n",
    "\n",
    "#Pre-Requisite : \n",
    "    #Kernel Python 3 (ipykernel) is required to run this notebook \n",
    "    #Required python version - Python 3.10.15 and its compatible Numpy , ScikitLearn libraries\n",
    "    \n",
    "#Old Name: 02_NB_EDA_pre_procs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fe11e4-be18-40c4-935b-9e33302b3167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n",
      "['Python 3.12.3']\n"
     ]
    }
   ],
   "source": [
    "#Check python version compatibility 3.10 or above is required\n",
    "!python -V\n",
    "python_version=!(python --version 2>&1)\n",
    "print (python_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99fe061-884a-46a6-8f7e-b694fab26c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Card\n"
     ]
    }
   ],
   "source": [
    "#Begin CARD\n",
    "print(\"Begin Card\")\n",
    "\n",
    "#User-Defined functions\n",
    "\n",
    "#To display the server time\n",
    "def showtime():\n",
    "    import time\n",
    "    \n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "    print(current_time)\n",
    "    return()\n",
    "\n",
    "#To save the output as CSV files from a Pandas Dataframe\n",
    "def savedata(tDF, tname) :\n",
    "    tpath = 'gcs://rdmai_data/'\n",
    "    tclensed = 'cleansed/'\n",
    "    tDF.to_csv(tpath+tclensed+tname)\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decbdf13-d2d6-4df8-9a9f-1650c64b1ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:37:35\n",
      "welcome\n",
      "Looking in indexes: https://europe-python.pkg.dev/artifact-registry-python-cache/virtual-python/simple/\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.3-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading https://europe-python.pkg.dev/artifact-registry-python-cache/virtual-python/py4j/py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
      "\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/py4j-0.10.9.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/pyspark already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/py4j already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/pyspark-3.5.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/share already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://europe-python.pkg.dev/artifact-registry-python-cache/virtual-python/simple/\n",
      "Collecting findspark\n",
      "  Downloading https://europe-python.pkg.dev/artifact-registry-python-cache/virtual-python/findspark/findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/findspark-2.0.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /mnt/dataproc/python/site-packages/findspark.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0mgcs://rdmai_data/raw/\n"
     ]
    }
   ],
   "source": [
    "showtime()\n",
    "\n",
    "#Library Declaration section - Installing or Initiating all required Python Libraries\n",
    "\n",
    "#Dataset Maths and OS\n",
    "#pip install numpy==1.24.3\n",
    "#pip install seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None) \n",
    "\n",
    "#Library Declaration section 1\n",
    "print(\"welcome\")\n",
    "\n",
    "#Constants declaration for the folder path for the local notebook path\n",
    "path = 'DataEAOLD/'\n",
    "raw = 'Raw/'\n",
    "curated = 'Curated/'\n",
    "cleansed = 'Cleansed/'\n",
    "\n",
    "#Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "path = 'gcs://rdmai_data/'\n",
    "raw = 'raw/'\n",
    "curated = 'curated/'\n",
    "cleansed = 'cleansed/'\n",
    "\n",
    "#Library Declaration section 3\n",
    "#building the Auto Arima model\n",
    "#import pmdarima as aa\n",
    "\n",
    "#Library Declaration section 4\n",
    "import re\n",
    "#from datetime_truncate import truncate\n",
    "from functools import reduce\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "#Library Declaration section 5\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#Library Declaration section 6\n",
    "import sklearn\n",
    "#print(sklearn.show_versions())\n",
    "\n",
    "#Library Declaration section 7\n",
    "#pip install imbalanced-learn --user\n",
    "#pip install imblearn --user\n",
    "#pip install -U threadpoolctl --user\n",
    "#import imblearn\n",
    "#print(imblearn.__version__)\n",
    "\n",
    "#Library Declaration section 8\n",
    "# Library Declarations - Model performance matrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Library Declaration section 9\n",
    "# Display configurations\n",
    "pd.set_option('display.max_columns', 400)\n",
    "#pd.set_option('max_rows', None)\n",
    "\n",
    "#Read input file\n",
    "import glob\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "import pandas\n",
    "\n",
    "!pip install pyspark | grep -v 'already satisfied'\n",
    "\n",
    "import pyspark\n",
    "\n",
    "#Install findspark\n",
    "!pip install findspark | grep -v 'already satisfied'\n",
    "\n",
    "# Import findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#import pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print (path+raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222091e9-3eff-4750-b833-0a4591a810c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load 24 years of WQ observations obtained from EA\n",
    "def loaddata() :\n",
    "    #Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "    path = 'gcs://rdmai_data/'\n",
    "    raw = 'raw/'\n",
    "    curated = 'curated/'\n",
    "    cleansed = 'cleansed/'\n",
    "\n",
    "    all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "    wqpath = 'Water_Quality_EA/'\n",
    "    #wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "    print (path+raw+wqpath )\n",
    "\n",
    "    #Read 1st Set from 2000 till 2004\n",
    "    df1 = pd.DataFrame()\n",
    "    csv_filenames1 = ['2000.csv', '2001.csv', '2002.csv', '2003.csv', '2004.csv']\n",
    "    #showtime()\n",
    "    df1 = pd.concat((pd.read_csv(path+raw+wqpath+f) for f in csv_filenames1), ignore_index=True)\n",
    "    #showtime()\n",
    "\n",
    "    #Read 1st Set from 2005 till 2009\n",
    "    df2 = pd.DataFrame()\n",
    "    csv_filenames2 = ['2005.csv', '2006.csv', '2007.csv', '2008.csv', '2009.csv']\n",
    "    #showtime()\n",
    "    df2 = pd.concat((pd.read_csv(path+raw+wqpath+f) for f in csv_filenames2), ignore_index=True)\n",
    "    #showtime()\n",
    "\n",
    "    #Read 3rd Set from 2010 till 2013\n",
    "    df3 = pd.DataFrame()\n",
    "    csv_filenames3 = ['2010.csv', '2011.csv', '2012.csv', '2013.csv', '2014.csv']\n",
    "\n",
    "    #showtime()\n",
    "    df3 = pd.concat((pd.read_csv(path+raw+wqpath+f) for f in csv_filenames3), ignore_index=True)\n",
    "    #showtime()\n",
    "\n",
    "    #Read 4th Set from 2014 till 2018\n",
    "    df4 = pd.DataFrame()\n",
    "    csv_filenames4 = ['2015.csv', '2016.csv', '2017.csv', '2018.csv', '2019.csv']\n",
    "\n",
    "    #showtime()\n",
    "    df4 = pd.concat((pd.read_csv(path+raw+wqpath+f) for f in csv_filenames4), ignore_index=True)\n",
    "    #showtime()\n",
    "\n",
    "    #Read 5th Set from 2019 till 2024\n",
    "    df5 = pd.DataFrame()\n",
    "    csv_filenames5 = ['2020.csv','2021.csv', '2022.csv', '2023.csv', '2024.csv']\n",
    "\n",
    "    #showtime()\n",
    "    df5 = pd.concat((pd.read_csv(path+raw+wqpath+f) for f in csv_filenames5), ignore_index=True)\n",
    "    #showtime()\n",
    "    \n",
    "    retdf = pd.DataFrame()\n",
    "    print (\"Concat df1 to df5 - Begin: \")\n",
    "    #showtime()\n",
    "    retdf = pd.concat((df1, df2, df3, df4, df5), ignore_index=True)\n",
    "    print (\"Concat df1 to df5 - End: \")\n",
    "    #showtime()\n",
    "    \n",
    "    #Clearing the memory used by the temporary datasets\n",
    "    del(df1, df2, df3, df4, df5)\n",
    "    print (\"deleted temp Pandas datasets\")\n",
    "           \n",
    "    return (retdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8b90e6-1b85-4f63-9918-9e13875acdbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Following 5 steps are reading the whole 24 year data into 5 different datasets\n",
    "# This steps was introduced to avoid high memory utilization and related failures\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6936569d-c10e-4345-87bb-33d0e61c48de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs://rdmai_data/raw/Water_Quality_EA/\n",
      "4951327\n",
      "3922008\n",
      "4587444\n",
      "4483915\n",
      "4265352\n",
      "22210046\n",
      "deleted temp Pandas datasets\n"
     ]
    }
   ],
   "source": [
    "#Main Read\n",
    "#1) Combining Files from the year 2000 to 2004\n",
    "\n",
    "#Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "path = 'gcs://rdmai_data/'\n",
    "raw = 'raw/'\n",
    "curated = 'curated/'\n",
    "cleansed = 'cleansed/'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "wqpath = 'Water_Quality_EA/'\n",
    "#wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "print (path+raw+wqpath )\n",
    "\n",
    "#Read 1st Set from 2000 till 2004\n",
    "csv_filenames1 = ['2000.csv', '2001.csv', '2002.csv', '2003.csv', '2004.csv']\n",
    "#showtime()\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1 = pd.read_csv(path+raw+wqpath+'2000.csv')\n",
    "print(len(df1))\n",
    "#showtime()\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2 = pd.read_csv(path+raw+wqpath+'2001.csv')\n",
    "print(len(df2))\n",
    "#showtime()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "df3 = pd.read_csv(path+raw+wqpath+'2002.csv')\n",
    "print(len(df3))\n",
    "#showtime()\n",
    "\n",
    "df4 = pd.DataFrame()\n",
    "df4 = pd.read_csv(path+raw+wqpath+'2003.csv')\n",
    "print(len(df4))\n",
    "#showtime()\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "df5 = pd.read_csv(path+raw+wqpath+'2004.csv')\n",
    "print(len(df5))\n",
    "#showtime()\n",
    "\n",
    "\n",
    "df_2000_2004 = pd.concat((df1, df2, df3, df4, df5), ignore_index=True)\n",
    "print(len(df_2000_2004))\n",
    "#Clearing the memory used by the temporary datasets\n",
    "del(df1, df2, df3, df4, df5)\n",
    "print (\"deleted temp Pandas datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e191d249-10b8-442e-8494-cfdd91a0739b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:41:10\n",
      "07:45:35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################\n",
    "#1) Write five years of data at once 2000-2004\n",
    "showtime()\n",
    "savedata(df_2000_2004, \"02_WQEA_2000_2004_Raw_New.csv\")\n",
    "showtime()\n",
    "del(df_2000_2004)\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e1f4c0-0101-4258-9265-6a8526db3c97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs://rdmai_data/raw/Water_Quality_EA/\n",
      "4117693\n",
      "4003373\n",
      "3709967\n",
      "3285780\n",
      "2787597\n",
      "17904410\n",
      "deleted temp Pandas datasets\n"
     ]
    }
   ],
   "source": [
    "#2) Combining Files from the year 2005 to 2009\n",
    "\n",
    "#Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "path = 'gcs://rdmai_data/'\n",
    "raw = 'raw/'\n",
    "curated = 'curated/'\n",
    "cleansed = 'cleansed/'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "wqpath = 'Water_Quality_EA/'\n",
    "#wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "print (path+raw+wqpath )\n",
    "\n",
    "#Read 1st Set from 2005 till 2009\n",
    "csv_filenames2 = ['2005.csv', '2006.csv', '2007.csv', '2008.csv', '2009.csv']\n",
    "#showtime()\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1 = pd.read_csv(path+raw+wqpath+'2005.csv')\n",
    "print(len(df1))\n",
    "#showtime()\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2 = pd.read_csv(path+raw+wqpath+'2006.csv')\n",
    "print(len(df2))\n",
    "#showtime()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "df3 = pd.read_csv(path+raw+wqpath+'2007.csv')\n",
    "print(len(df3))\n",
    "#showtime()\n",
    "\n",
    "df4 = pd.DataFrame()\n",
    "df4 = pd.read_csv(path+raw+wqpath+'2008.csv')\n",
    "print(len(df4))\n",
    "#showtime()\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "df5 = pd.read_csv(path+raw+wqpath+'2009.csv')\n",
    "print(len(df5))\n",
    "#showtime()\n",
    "\n",
    "\n",
    "df_2005_2009 = pd.concat((df1, df2, df3, df4, df5), ignore_index=True)\n",
    "print(len(df_2005_2009))\n",
    "#Clearing the memory used by the temporary datasets\n",
    "del(df1, df2, df3, df4, df5)\n",
    "print (\"deleted temp Pandas datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2acf36e2-8c38-465e-8e12-16c9bc6ca4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:00:33\n",
      "08:04:04\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "#2) Write five years of data at once 2005-2009\n",
    "showtime()\n",
    "savedata(df_2005_2009, \"02_WQEA_2005_2009_Raw_New.csv\")\n",
    "showtime()\n",
    "del(df_2005_2009)\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a00803-a7f1-41dd-b271-26d8339ae96a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs://rdmai_data/raw/Water_Quality_EA/\n",
      "2610447\n",
      "2657107\n",
      "2762712\n",
      "2852883\n",
      "2380052\n",
      "13263201\n",
      "deleted temp Pandas datasets\n"
     ]
    }
   ],
   "source": [
    "#3) Combining Files from the year 2010 to 2014\n",
    "\n",
    "#Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "path = 'gcs://rdmai_data/'\n",
    "raw = 'raw/'\n",
    "curated = 'curated/'\n",
    "cleansed = 'cleansed/'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "wqpath = 'Water_Quality_EA/'\n",
    "#wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "print (path+raw+wqpath )\n",
    "\n",
    "#Read 1st Set from 2010 till 2014\n",
    "csv_filenames3 = ['2010.csv', '2011.csv', '2012.csv', '2013.csv', '2014.csv']\n",
    "#showtime()\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1 = pd.read_csv(path+raw+wqpath+'2010.csv')\n",
    "print(len(df1))\n",
    "#showtime()\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2 = pd.read_csv(path+raw+wqpath+'2011.csv')\n",
    "print(len(df2))\n",
    "#showtime()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "df3 = pd.read_csv(path+raw+wqpath+'2012.csv')\n",
    "print(len(df3))\n",
    "#showtime()\n",
    "\n",
    "df4 = pd.DataFrame()\n",
    "df4 = pd.read_csv(path+raw+wqpath+'2013.csv')\n",
    "print(len(df4))\n",
    "#showtime()\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "df5 = pd.read_csv(path+raw+wqpath+'2014.csv')\n",
    "print(len(df5))\n",
    "#showtime()\n",
    "\n",
    "\n",
    "df_2010_2014 = pd.concat((df1, df2, df3, df4, df5), ignore_index=True)\n",
    "print(len(df_2010_2014))\n",
    "#Clearing the memory used by the temporary datasets\n",
    "del(df1, df2, df3, df4, df5)\n",
    "print (\"deleted temp Pandas datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51b19f8f-e543-4f97-a534-9906a1b569b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:10:54\n",
      "08:13:40\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "#3) Write five years of data at once 2010-2014\n",
    "showtime()\n",
    "savedata(df_2010_2014, \"02_WQEA_2010_2014_Raw_New.csv\")\n",
    "showtime()\n",
    "del(df_2010_2014)\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e87fcf4-619d-4a23-96a4-3667365a2782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs://rdmai_data/raw/Water_Quality_EA/\n",
      "2217068\n",
      "2097613\n",
      "1851074\n",
      "1567779\n",
      "1678148\n",
      "9411682\n",
      "deleted temp Pandas datasets\n"
     ]
    }
   ],
   "source": [
    "#4) Combining Files from the year 2015 to 2019\n",
    "\n",
    "#Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "path = 'gcs://rdmai_data/'\n",
    "raw = 'raw/'\n",
    "curated = 'curated/'\n",
    "cleansed = 'cleansed/'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "wqpath = 'Water_Quality_EA/'\n",
    "#wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "print (path+raw+wqpath )\n",
    "\n",
    "#Read 1st Set from 2015 till 2019\n",
    "csv_filenames4 = ['2015.csv', '2016.csv', '2017.csv', '2018.csv', '2019.csv']\n",
    "#showtime()\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1 = pd.read_csv(path+raw+wqpath+'2015.csv')\n",
    "print(len(df1))\n",
    "#showtime()\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2 = pd.read_csv(path+raw+wqpath+'2016.csv')\n",
    "print(len(df2))\n",
    "#showtime()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "df3 = pd.read_csv(path+raw+wqpath+'2017.csv')\n",
    "print(len(df3))\n",
    "#showtime()\n",
    "\n",
    "df4 = pd.DataFrame()\n",
    "df4 = pd.read_csv(path+raw+wqpath+'2018.csv')\n",
    "print(len(df4))\n",
    "#showtime()\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "df5 = pd.read_csv(path+raw+wqpath+'2019.csv')\n",
    "print(len(df5))\n",
    "#showtime()\n",
    "\n",
    "\n",
    "df_2015_2019 = pd.concat((df1, df2, df3, df4, df5), ignore_index=True)\n",
    "print(len(df_2015_2019))\n",
    "#Clearing the memory used by the temporary datasets\n",
    "del(df1, df2, df3, df4, df5)\n",
    "print (\"deleted temp Pandas datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45ba1be3-4808-45b7-b499-3160007b95a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:18:12\n",
      "08:20:17\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "#4) Write five years of data at once 2015-2019\n",
    "showtime()\n",
    "savedata(df_2015_2019, \"02_WQEA_2015_2019_Raw_New.csv\")\n",
    "showtime()\n",
    "del(df_2015_2019)\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb94ac6-d82e-42a3-8c2e-f625e23418c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs://rdmai_data/raw/Water_Quality_EA/\n",
      "668453\n",
      "1229860\n",
      "1513944\n",
      "1726183\n",
      "542881\n",
      "5681321\n",
      "deleted temp Pandas datasets\n"
     ]
    }
   ],
   "source": [
    "#5) Combining Files from the year 2020 to 2024\n",
    "\n",
    "#Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "path = 'gcs://rdmai_data/'\n",
    "raw = 'raw/'\n",
    "curated = 'curated/'\n",
    "cleansed = 'cleansed/'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "wqpath = 'Water_Quality_EA/'\n",
    "#wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "print (path+raw+wqpath )\n",
    "\n",
    "#Read 1st Set from 2020 till 2024\n",
    "csv_filenames5 = ['2020.csv', '2021.csv', '2022.csv', '2023.csv', '2024.csv']\n",
    "#showtime()\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1 = pd.read_csv(path+raw+wqpath+'2020.csv')\n",
    "print(len(df1))\n",
    "#showtime()\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2 = pd.read_csv(path+raw+wqpath+'2021.csv')\n",
    "print(len(df2))\n",
    "#showtime()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "df3 = pd.read_csv(path+raw+wqpath+'2022.csv')\n",
    "print(len(df3))\n",
    "#showtime()\n",
    "\n",
    "df4 = pd.DataFrame()\n",
    "df4 = pd.read_csv(path+raw+wqpath+'2023.csv')\n",
    "print(len(df4))\n",
    "#showtime()\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "df5 = pd.read_csv(path+raw+wqpath+'2024.csv')\n",
    "print(len(df5))\n",
    "#showtime()\n",
    "\n",
    "\n",
    "df_2020_2024 = pd.concat((df1, df2, df3, df4, df5), ignore_index=True)\n",
    "print(len(df_2020_2024))\n",
    "#Clearing the memory used by the temporary datasets\n",
    "del(df1, df2, df3, df4, df5)\n",
    "print (\"deleted temp Pandas datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec0a1456-f88d-4763-9809-f60c11ffbc45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:32:39\n",
      "08:33:50\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "#5) Write five years of data at once 2020-2024\n",
    "showtime()\n",
    "savedata(df_2020_2024, \"02_WQEA_2020_2024_Raw_New.csv\")\n",
    "showtime()\n",
    "del(df_2020_2024)\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21fb2e8-d8ca-4cd3-b56d-4414809da048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End CARD\n",
    "#In line comments completed 09-May-2025"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-chem2-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-env-chem2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
