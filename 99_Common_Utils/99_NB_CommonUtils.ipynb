{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d698a8c-5327-421a-93a1-eb56b8ba5922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip list #To obtain requirements.txt for Python 3 ipykernel (Python version 3.10.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826beee7-3437-4ef9-a98a-c494c9e12b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12238a4c-3958-439c-9dde-c2863d74b8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intalling required libraries and utilities.....\n",
      "Uses Python 3 (ipykernel) (Local)\n"
     ]
    }
   ],
   "source": [
    "# Welcome To Common Utility Handle\"\n",
    "print (\"Intalling required libraries and utilities.....\")\n",
    "print(\"Uses Python 3 (ipykernel) (Local)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9bf92fd-f2ca-4549-bd2a-cb62c728eb58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.15\n",
      "['Python 3.10.15']\n"
     ]
    }
   ],
   "source": [
    "!python -V\n",
    "python_version=!(python --version 2>&1)\n",
    "print (python_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791526d7-27c3-4379-afda-b82124d52eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Import the warning module.\n",
    "warnings.filterwarnings('ignore', message='not allowed')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503b3588-ba85-4690-a88a-bd5ed8b02699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec6934a7-1943-436f-bd52-cf3a9f04bffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r 'requirements.txt' | grep -v 'already satisfied'\n",
    "#!pip install -r requirements | grep -v 'Requirement already satisfied'\n",
    "#%run \"requirements.txt\"\n",
    "#!find -name \"requirements.txt\"\n",
    "#!pip freeze > requirements.txt\n",
    "#!pip install -r requirements.txt | grep -v 'Requirement already satisfied'\n",
    "#!pip3 install -r requirements.txt\n",
    "#%run \"requirements.ipynb\"\n",
    "#python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffaffb85-1af1-4e88-959e-caa8e0c4ddb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pkg_resources\\ninstalled_packages = [str(d) for d in pkg_resources.working_set]\\n#print(dists)\\nif \\'spacy\\' not in  installed_packages:\\n    print (\"No Spacy\")\\nelse:\\n    print(\"Spacy installed\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pip\n",
    "#!pip -V\n",
    "#from pip import get_installed_distributions\n",
    "#installed_packages = pip.get_installed_distributions()\n",
    "#print(installed_packages)\n",
    "\n",
    "'''\n",
    "import pkg_resources\n",
    "installed_packages = [str(d) for d in pkg_resources.working_set]\n",
    "#print(dists)\n",
    "if 'spacy' not in  installed_packages:\n",
    "    print (\"No Spacy\")\n",
    "else:\n",
    "    print(\"Spacy installed\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59533c55-ab08-4ca2-81c4-bbc9dc07e3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Commented on 22 April 2025 as import sklearn in the following steps gives error Begin\\n#Installing required libraries\\n!pip install 'scipy<1.13.0' --user | grep -v 'already satisfied'\\n#!pip install spacy --user | grep -v 'already satisfied'\\n#!pip install -U spacy | grep -v 'already satisfied' #to Upgrade\\n\\n#!pip3 install spacy\\n#!pip3 install -U spacy #to Upgrade\\n\\n#!python -m pip install spacy\\n#!python3 -m pip install spacy\\n\\n# Reference: https://spacy.io/models/en\\n!python -m spacy download en_core_web_sm | grep -v 'already satisfied' #size  12 MB, F1Scores: Sentences-91%, NamedEntities-84% \\n#!python -m spacy download en_core_web_md  #size  31 MB, F1Scores: Sentences-91%, NamedEntities-85%\\n!python -m spacy download en_core_web_lg | grep -v 'already satisfied' #size 382 MB, F1Scores: Sentences-91%, NamedEntities-86%\\n#!python -m spacy download en_core_web_trf #size 436 MB, F1Scores: Sentences-89%, NamedEntities-90%\\n\\n#Commented on 22 April 2025 as import sklearn in the following steps gives error End\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Commented on 22 April 2025 as import sklearn in the following steps gives error Begin\n",
    "#Installing required libraries\n",
    "!pip install 'scipy<1.13.0' --user | grep -v 'already satisfied'\n",
    "#!pip install spacy --user | grep -v 'already satisfied'\n",
    "#!pip install -U spacy | grep -v 'already satisfied' #to Upgrade\n",
    "\n",
    "#!pip3 install spacy\n",
    "#!pip3 install -U spacy #to Upgrade\n",
    "\n",
    "#!python -m pip install spacy\n",
    "#!python3 -m pip install spacy\n",
    "\n",
    "# Reference: https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_sm | grep -v 'already satisfied' #size  12 MB, F1Scores: Sentences-91%, NamedEntities-84% \n",
    "#!python -m spacy download en_core_web_md  #size  31 MB, F1Scores: Sentences-91%, NamedEntities-85%\n",
    "!python -m spacy download en_core_web_lg | grep -v 'already satisfied' #size 382 MB, F1Scores: Sentences-91%, NamedEntities-86%\n",
    "#!python -m spacy download en_core_web_trf #size 436 MB, F1Scores: Sentences-89%, NamedEntities-90%\n",
    "\n",
    "#Commented on 22 April 2025 as import sklearn in the following steps gives error End\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cffeca31-4572-4ef5-9f1f-93fe135ace13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif (installed_packages.count(\\'spacy\\') == 0):\\n    print (\"No Spacy\")\\nelse:\\n    print(\"Spacy installed\")\\n\\n#type(installed_packages)\\n#str(tuple(installed_packages)).startswith(\\'spacy\\')\\n#installed_packages\\nteststring = \\'spacy \\'\\nlist(filter(installed_packages.startswith, teststring)) != []\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if (installed_packages.count('spacy') == 0):\n",
    "    print (\"No Spacy\")\n",
    "else:\n",
    "    print(\"Spacy installed\")\n",
    "\n",
    "#type(installed_packages)\n",
    "#str(tuple(installed_packages)).startswith('spacy')\n",
    "#installed_packages\n",
    "teststring = 'spacy '\n",
    "list(filter(installed_packages.startswith, teststring)) != []\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2ed3ce-5e81-471b-92c7-5f3aa36ce7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Installing required libraries\n",
    "#!pip install -U periodictable #to uninsall\n",
    "!pip install periodictable  | grep -v 'already satisfied'\n",
    "\n",
    "!pip install chemlib  | grep -v 'already satisfied'\n",
    "#!pip install -U chemlib #to uninsall\n",
    "\n",
    "!pip install pyspark  | grep -v 'already satisfied'\n",
    "\n",
    "# Library Instalations\n",
    "!pip install imbalanced-learn --user  | grep -v 'already satisfied'\n",
    "!pip install imblearn --user  | grep -v 'already satisfied'\n",
    "#!pip install -U threadpoolctl --user #uninstall\n",
    "!pip install threadpoolctl --user  | grep -v 'already satisfied'\n",
    "\n",
    "!pip install xgboost  | grep -v 'already satisfied'\n",
    "!pip install --upgrade xgboost  | grep -v 'already satisfied'\n",
    "\n",
    "#Install findspark\n",
    "!pip install findspark  | grep -v 'already satisfied'\n",
    "!pip install pyspark  | grep -v 'already satisfied'\n",
    "\n",
    "#Library Declaration section 2\n",
    "#Dataset Maths and OS\n",
    "#pip install numpy==1.24.3\n",
    "!pip install seaborn  | grep -v 'already satisfied'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a6e9d13-6094-4ac3-a115-7acee97c23ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge spacy\n",
    "#!python3 -m pip install spacy\n",
    "#Sample implementation for parsing input text by sentencewise\n",
    "#from spacy.lang.en.examples import sentences # commented on Dec 5 2024 as error out\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "#doc = nlp(sentences[0])\n",
    "#print(doc.text)\n",
    "#for token in doc:\n",
    "#    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "#import spacy # commented on Dec 5 2024 as error out\n",
    "#import en_core_web_sm # commented on Dec 5 2024 as error out\n",
    "#import en_core_web_lg # commented on Dec 5 2024 as error out\n",
    "\n",
    "#nlp = en_core_web_sm.load()\n",
    "#nlp = en_core_web_lg.load() # commented on Dec 5 2024 as error out\n",
    "#doc = nlp(u\"Alkalinity (to pH 4.5 as CaCO3)\")\n",
    "#doc.has_unknown_spaces\n",
    "#doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "984d5d2a-03dd-4205-951d-2cff54823110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import periodictable\n",
    "#from en_core_web_lg.Elements.core import periodic_table, Element, Isotope\n",
    "from periodictable import *\n",
    "\n",
    "#https://github.com/lmmentel/awesome-python-chemistry?tab=readme-ov-file#awesome-python-chemistry-\n",
    "#Python based Chemical Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "038cdcf7-9ffa-46dd-bf5d-9f1bf51d6862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#i=0\n",
    "#for el in elements:  # lists the element symbols\n",
    "#    i=i+1\n",
    "#    print(\"%s %s %s\"%(i, el.symbol, el.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "181612e7-d6a8-4174-aaf0-055a02657df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip uninstall scikit-learn\n",
    "\n",
    "#!pip install scikit-learn | grep -v 'already'\n",
    "\n",
    "#!pip install scikit-learn --user | grep -v 'already'\n",
    "\n",
    "#!pip install numpy --upgrade\n",
    "\n",
    "#!pip uninstall cupy-cuda11x --y\n",
    "\n",
    "#!pip uninstall cupy-cuda12x --y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9024988-156b-4bd1-8b2f-856499677016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "#print(sklearn.show_versions())\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abf9a9f8-8902-469f-a9df-2d16b9c0a8de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#User-Defined functions\n",
    "def showtime():\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    \n",
    "    t = time.localtime()\n",
    "    d = datetime.now()\n",
    "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "    current_date = d.strftime(\"%d %B %Y\")\n",
    "    print(current_date, current_time)\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec6b1e5-8f11-425d-9d2b-2994e74d73e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stime(flag=True):\n",
    "    import time\n",
    "    from time import strftime\n",
    "\n",
    "    while True:\n",
    "        print (strftime(\"%m/%d/%Y %H:%M:%S\"), end=\"\", flush=True)\n",
    "        print(\"\\r\", end=\"\", flush=True)\n",
    "        time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72ec34ce-cfa3-4ab6-a133-6cf055eced15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#showtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89fc03b3-653f-4976-a05a-643531b147ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 April 2025 21:41:17\n",
      "welcome\n",
      "gcs://rdmai_data/raw/\n"
     ]
    }
   ],
   "source": [
    "showtime()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None) \n",
    "\n",
    "#Library Declaration section 1\n",
    "print(\"welcome\")\n",
    "\n",
    "#Constants declaration for the folder path for the local notebook path\n",
    "path = 'DataEAOLD/'\n",
    "raw = 'Raw/'\n",
    "curated = 'Curated/'\n",
    "cleansed = 'Cleansed/'\n",
    "\n",
    "#Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "path = 'gcs://rdmai_data/'\n",
    "raw = 'raw/'\n",
    "curated = 'curated/'\n",
    "cleansed = 'cleansed/'\n",
    "\n",
    "#Library Declaration section 3\n",
    "#building the Auto Arima model\n",
    "#import pmdarima as aa\n",
    "\n",
    "#Library Declaration section 4\n",
    "import re\n",
    "#from datetime_truncate import truncate\n",
    "from functools import reduce\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "#Library Declaration section 5\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#Library Declaration section 6\n",
    "import sklearn\n",
    "#print(sklearn.show_versions())\n",
    "\n",
    "#Library Declaration section 7\n",
    "#pip install imbalanced-learn --user\n",
    "#pip install imblearn --user\n",
    "#pip install -U threadpoolctl --user\n",
    "#import imblearn\n",
    "#print(imblearn.__version__)\n",
    "\n",
    "#Library Declaration section 8\n",
    "# Library Declarations - Model performance matrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Library Declaration section 9\n",
    "# Display configurations\n",
    "pd.set_option('display.max_columns', 400)\n",
    "#pd.set_option('max_rows', None)\n",
    "\n",
    "#Read input file\n",
    "import glob\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "#import pandas\n",
    "\n",
    "#!pip install pyspark\n",
    "import pyspark\n",
    "\n",
    "#Install findspark\n",
    "#!pip install findspark \n",
    "\n",
    "# Import findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#import pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print (path+raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "167e47ee-b9e4-4f05-b6d7-c4b774619edd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Card\n"
     ]
    }
   ],
   "source": [
    "#User-Defined functions to save dataset to pandas dataframe\n",
    "\n",
    "def fnull(df):\n",
    "    # Code to find out the null value \n",
    "    from pyspark.sql.functions import col, count\n",
    "    #df = df.select(*(col(c).cast('float').alias(c) for c in df.columns))\n",
    "    display(df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]))\n",
    "    \n",
    "def addindex(df):\n",
    "    # Set monotonically increasing ID\n",
    "    from pyspark.sql.functions import monotonically_increasing_id\n",
    "    df = df.withColumn(\"index_id\", monotonically_increasing_id())\n",
    "    display(df.limit(2))\n",
    "\n",
    "#START CARD\n",
    "print(\"Begin Card\")\n",
    "\n",
    "def savedata(tDF, tname, tpath = 'gcs://rdmai_data/') :\n",
    "    #tpath = 'gcs://rdmai_data/'\n",
    "    tclensed = 'cleansed/'\n",
    "    tDF.to_csv(tpath+tclensed+tname)\n",
    "    print(\"saved, Location: \", tpath+tclensed+tname)\n",
    "    return(\"saved, Location: \", tpath+tclensed+tname)\n",
    "\n",
    "def savecsv(tDF, tname, tpath = 'gcs://rdmai_data/') :\n",
    "    #tpath = 'gcs://rdmai_data/'\n",
    "    tclensed = 'cleansed/'\n",
    "    tDF.to_csv(tpath+tclensed+tname)\n",
    "    return(\"saved, Location: \", tpath+tclensed+tname)\n",
    "\n",
    "def saveparquet(dffinal, fpath, fname, root = 'gcs://rdmai_data/') :\n",
    "    root = 'gcs://rdmai_data/'\n",
    "    #tclensed = 'cleansedParquet/'\n",
    "    dffinal = spark.createDataFrame(dffinal)\n",
    "    dffinal.write.parquet(\"root+fpath+fname\", mode = 'Overwrite')\n",
    "    return()\n",
    "\n",
    "# Release memory used for the temporary data sets df1 and df2 in the above readCSV\n",
    "def clearMem(df1):\n",
    "    import ctypes\n",
    "    # Load the malloc_trim function from the C standard library\n",
    "    malloc_trim = ctypes.CDLL(\"libc.so.6\").malloc_trim\n",
    "    \n",
    "    print(f\"Memory usage before calling malloc_trim:\\\n",
    "        {df1.memory_usage().sum()} bytes\")\n",
    "        #{df1.memory_usage().sum(), df2.memory_usage().sum()} bytes\")\n",
    "    del df1, df2\n",
    "    malloc_trim(0)\n",
    "\n",
    "#Main Read\n",
    "#Load data from csv files\n",
    "#def loaddata(fpath, fname) : #added a path parameter as optional paramter Jan 2 2025 - Pasu\n",
    "def loaddata(fpath, fname, path = 'gcs://rdmai_data/') : #added a path parameter as optional paramter  Jan 2 2025 - Pasu\n",
    "    #Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "    #path = 'gcs://rdmai_data/' #added a path parameter as optional paramter Jan 2 2025, hence commented here - Pasu\n",
    "\n",
    "    #all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "    #wqpath = 'Water_Quality_EA/'\n",
    "    ##wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "    print (path+fpath+fname)\n",
    "\n",
    "    #Read full clensed Set where the data is from 2000 till 2004\n",
    "    retdf = pd.DataFrame()\n",
    "    #showtime()\n",
    "    retdf = pd.read_csv(path+fpath+fname)\n",
    "    #showtime()\n",
    "           \n",
    "    return (retdf)\n",
    "\n",
    "def loaddata_withindex(fpath, fname, indexcol) :\n",
    "    #Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "    #path = 'gcs://rdmai_data/' #CTS GCP\n",
    "    path = 'gcs://rdmai_dev_data/' #NW GCP\n",
    "\n",
    "    #all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "    #wqpath = 'Water_Quality_EA/'\n",
    "    ##wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "    print (path+fpath+fname)\n",
    "\n",
    "    #Read full clensed Set where the data is from 2000 till 2004\n",
    "    retdf = pd.DataFrame()\n",
    "    #showtime()\n",
    "    retdf = pd.read_csv(path+fpath+fname, index_col=indexcol)\n",
    "    #showtime()\n",
    "           \n",
    "    return (retdf)\n",
    "\n",
    "\n",
    "#Load data from parquet dataset \n",
    "def loaddata2(fpath, fname) :\n",
    "    #Constants declaration for the folder path for files stored under Google Cloud Storage \n",
    "    path = 'gs://rdmai_data/'\n",
    "\n",
    "    #all_files = glob.glob(os.path.join(path + raw, \"*.csv\"))\n",
    "    #wqpath = 'Water_Quality_EA/'\n",
    "    ##wqpath = '' #Make this commented when reading from Google Cloud Storage\n",
    "\n",
    "    print (path+fpath+fname)\n",
    "\n",
    "    #Read full clensed Set where the data is from 2000 till 2004\n",
    "    #retdf = spark.DataFrame()\n",
    "    #showtime()\n",
    "    retdf = spark.read.parquet(path+fpath+fname, engine = 'fastparquet')\n",
    "    #showtime()           \n",
    "    return (retdf)\n",
    "\n",
    "def drawheatmap(dftemp):\n",
    "    plt.figure(figsize=(21, 12))\n",
    "    sns.heatmap(dftemp.corr(), annot=True, linewidths=1.5, cmap=\"coolwarm\", fmt='.1g', square=False)\n",
    "    plt.title(\"Correlation Matrix - Heatmap\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d90e556-d64a-4e92-88ed-8727cd387b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Outlier deduction \n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(dfp['monthly_Spill_Duration'])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(dfp[\"monthly_RainFall_Duration\"])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(dfp[\"monthly_Spill_12_24_Count\"])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.distplot(dfp['monthly_Spilled_Days'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(dfp['monthly_Rained_Days'])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(dfp['monthly_NoSpill_Days'])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(dfp['monthly_NoRain_Days'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.distplot(dfp['Month'])\n",
    "\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "#Learning from the above picture that the data is right skewed, we may use IQR method to find and remove outliers\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.boxplot(dfp['monthly_Spill_Duration'])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(dfp['monthly_RainFall_Duration'])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.boxplot(dfp['monthly_Spill_12_24_Count'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(dfp['monthly_Spilled_Days'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.boxplot(dfp['monthly_Rained_Days'])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(dfp['monthly_NoSpill_Days'])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.boxplot(dfp['monthly_NoRain_Days'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(dfp['Month'])\n",
    "plt.show()\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e559ecc0-a05c-49c0-8161-d1a894ba256a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Converting the Fields -> \"Device_Name\" and \"Permit_Site_Name\" into numberical values.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(dfp['Device_Name_LEncoded'])\n",
    "#dfp.drop(\"Device_Name\", axis=1, inplace=True)\n",
    "dfp[\"Device_Name_LEncoded\"] = label\n",
    "\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(dfp['Permit_Site_Name_LEncoded'])\n",
    "#dfp.drop(\"Permit_Site_Name_LEncoded\", axis=1, inplace=True)\n",
    "dfp[\"Permit_Site_Name_LEncoded\"] = label\n",
    "\n",
    "display(dfp.head(5))\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dce726bb-4ac4-4e7d-944f-0b022ca464ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def configure_widgets():\n",
    "  dbutils.widgets.removeAll()\n",
    "  #abfss://wastewater@stuudnadevuks01.dfs.core.windows.net/Cleansed/DS/merged_Data/\n",
    "  dbutils.widgets.text('File_Path', 'abfss://wastewater@stuudnadevuks01.dfs.core.windows.net/Cleansed/DS/', 'File_Path')\n",
    "  #dbutils.widgets.text('File_Name', 'merged_Data/', 'File name')\n",
    "  \n",
    "  #Parquet File name\n",
    "  dbutils.widgets.text('File_Name', 'UC1_df_inde_dep_feed_part4/', 'File name')\n",
    "\n",
    "configure_widgets()\n",
    "param_FILE_PATH = getArgument('File_Path')\n",
    "param_FILE_NAME = getArgument('File_Name')\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6de7f9c9-92ae-4f54-9f8f-dd6074613912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reqcols = 'sno', 'unique_ID', 'samplingPoint', 'samplingPoint_notation', 'samplingPoint_name', 'sampleDateTime', \\\n",
    "            'determinand_name', 'determinand_definition', 'determinand_notation', 'codedResultInterpretation_interpretation', \\\n",
    "            'determinand_unit_name', 'sampledMaterialType_name', 'purpose_name', 'isComplianceSample', 'result', \\\n",
    "            'samplingPoint_easting', 'samplingPoint_northing'\n",
    "\n",
    "#reqcols = 'samplingPoint', 'samplingPoint_notation', 'samplingPoint_name', 'sampleDateTime', \\\n",
    "#            'determinand_name', 'determinand_definition', 'determinand_notation', 'codedResultInterpretation_interpretation', \\\n",
    "#            'determinand_unit_name', 'sampledMaterialType_name', 'purpose_name', 'isComplianceSample', 'result', \\\n",
    "#            'samplingPoint_easting', 'samplingPoint_northing'\n",
    "\n",
    "#'unique_ID', '0', 'samplingPoint', 'samplingPoint_notation', 'samplingPoint_name', 'sampleDateTime',\n",
    "#'determinand_name','determinand_definition', 'determinand_notation', 'codedResultInterpretation_interpretation',\n",
    "#'determinand_unit_name', 'sampledMaterialType_name', 'purpose_name', 'isComplianceSample', 'result',\n",
    "#'samplingPoint_easting', 'samplingPoint_northing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ea1fdc7-9318-44dc-a3a4-bc028cee6c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Kmeans sample: https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/\n",
    "#visualization library Elbow and Silhoutte scores\n",
    "#https://www.scikit-yb.org/en/latest/quickstart.html#installation\n",
    "! pip install yellowbrick  | grep -v 'already satisfied'\n",
    "! pip install -U yellowbrick | grep -v 'already satisfied' #Upgrade latest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41946256-e739-4d72-8356-a87da37c08e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#END CARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc9d139f-b181-4f90-847d-ce2c62770a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:41:37.457437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745358097.716040   19743 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745358097.791891   19743 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745358098.409664   19743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745358098.409705   19743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745358098.409707   19743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745358098.409710   19743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-22 21:41:38.469527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Install Transformers Chemberta\n",
    "!pip install transformers | grep -v 'already satisfied'\n",
    "#!pip install pyspark transformers findspark pyspark regex torchvision\n",
    "\n",
    "!pip install rdkit scikit-learn  | grep -v 'already satisfied'\n",
    "!pip install scispacy  | grep -v 'already satisfied'\n",
    "!pip install torch  | grep -v 'already satisfied'\n",
    "\n",
    "#!conda install chemdataextractor\n",
    "#!pip install chemdataextractor #This library requires Python 3.5 max but our python version is 3.10 so can't be installed in vertex ai\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31823153-0ecf-4254-a100-ae1a06d6ee45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm | grep -v 'already satisfied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bb0510b-dad4-4baf-8398-1aa56aad88f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Completed intalling required libraries and utilities ||\n"
     ]
    }
   ],
   "source": [
    "print (\"|| Completed intalling required libraries and utilities ||\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55e895ee-f31e-4e7d-ba79-cefdc986dfd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "035b6b88-94ab-4535-8074-e86a537205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For drawing sattelitte maps\n",
    "! pip install geopandas  | grep -v 'Requirement already satisfied'\n",
    "! pip install contextily  | grep -v 'Requirement already satisfied'\n",
    "! pip install pyproj | grep -v 'Requirement already satisfied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed688b64-a4c5-4295-addd-784f06706daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost | grep -v 'Requirement already satisfied'\n",
    "!pip install permetrics | grep -v 'Requirement already satisfied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5765ed8-7f3d-42c2-a319-44822ac8f7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "from pyproj import Transformer\n",
    "import geodatasets\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import osmnx as ox\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from permetrics.regression import RegressionMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "604ba37e-1bc4-4c3e-a9f9-380401b33f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name} RMSE: {rmse:.2f}\")\n",
    "    print(f\"{model_name} R²: {r2:.2f}\")\n",
    "    return y_pred\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model_adv(model, X_test, y_test, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    #adjusted_r2 = 1-(1-r2(actual,predicted))*(rowcount-1)/(rowcount-featurecount)\n",
    "    rowcount = len(y_test)\n",
    "    featurecount = X_test.shape[1]-1\n",
    "    adjusted_r2 = 1- (1-r2) * (rowcount-1)/(rowcount-featurecount)\n",
    "    \n",
    "    #Calculate NRMSE value\n",
    "    evaluator = RegressionMetric(y_test, y_pred)\n",
    "    nrmse = evaluator.normalized_root_mean_square_error()\n",
    "  \n",
    "    print(\"Metrics for the models' Prediction \")\n",
    "    print(f\"{model_name} RMSE: {rmse:.2f}\")\n",
    "    print(f\"{model_name} R²: {r2:.2f}\")\n",
    "    print(f\"{model_name} adjusted Adjusted R²: {adjusted_r2:.2f}\")\n",
    "    print(f\"{model_name} NRMSE: {nrmse:.2f}\")\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71b6f66d-9459-4597-8ba0-3c398319965e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to evaluate model using Qmetrics\n",
    "# Q-model metrics are designed for spatio-temporal data.\n",
    "#  These metrics assess the explained variance, bias, and variability of predictions\n",
    "def evaluate_model_q_metrics(observed, predicted, model_name=\"Model\"):\n",
    "    \n",
    "    #Step 1: Import Libraries and Load Data\n",
    "    #import numpy as np\n",
    "    #import pandas as pd\n",
    "\n",
    "    # Example: Create a sample dataset\n",
    "    # Actual values (observed) and predicted values\n",
    "    #data = pd.DataFrame({\n",
    "    #    'observed': [2.3, 2.7, 3.1, 2.9, 3.6, 2.5],\n",
    "    #    'predicted': [2.4, 2.5, 3.2, 3.0, 3.4, 2.6]\n",
    "    #})\n",
    "    \n",
    "    #Step 2: Define Q-Metrics Functions\n",
    "    #1. Explained Variance ():\n",
    "    #Q_1 = 1 - \\frac{\\sum (y_{\\text{obs}} - y_{\\text{pred}})^2}{\\sum (y_{\\text{obs}} - \\bar{y}_{\\text{obs}})^2}\n",
    "\n",
    "    #2. Bias ():\n",
    "    #Q_2 = \\frac{\\sum (y_{\\text{obs}} - y_{\\text{pred}})}{\\sum (y_{\\text{obs}})}\n",
    "\n",
    "    #3. Variance Ratio ():\n",
    "    #Q_3 = \\frac{\\text{Var}(y_{\\text{pred}})}{\\text{Var}(y_{\\text{obs}})}\n",
    "\n",
    "    # Mean of observed values\n",
    "    mean_observed = np.mean(observed)\n",
    "    \n",
    "    # Residual sum of squares\n",
    "    ss_res = np.sum((observed - predicted) ** 2)\n",
    "    \n",
    "    # Total sum of squares\n",
    "    ss_tot = np.sum((observed - mean_observed) ** 2)\n",
    "    \n",
    "    # Q1: Explained Variance\n",
    "    q1 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    # Q2: Bias\n",
    "    q2 = np.sum(observed - predicted) / np.sum(observed)\n",
    "    \n",
    "    # Q3: Variance Ratio\n",
    "    var_observed = np.var(observed, ddof=1)  # Sample variance\n",
    "    var_predicted = np.var(predicted, ddof=1)  # Sample variance\n",
    "    q3 = var_predicted / var_observed\n",
    "\n",
    "    # Extract observed and predicted values\n",
    "    #observed = data['observed'].values\n",
    "    #predicted = data['predicted'].values\n",
    "\n",
    "    # Compute Q-metrics\n",
    "    #q1, q2, q3 = evaluate_model_q_metrics(observed, predicted)\n",
    "\n",
    "    print(f\"Q1 {model_name} (Explained Variance): {q1:.3f}\")\n",
    "    print(f\"Q2 {model_name} (Bias): {q2:.3f}\")\n",
    "    print(f\"Q3 {model_name} (Variance Ratio): {q3:.3f}\")\n",
    "    \n",
    "    return q1, q2, q3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6413d53-63ef-423b-9c5f-a67e263c04eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
